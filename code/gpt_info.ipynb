{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import argparse\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(r'api.env')\n",
    "\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Please ensure the .env file is correct and the key is properly set.\")\n",
    "\n",
    "\n",
    "openai.api_key = api_key\n",
    "\n",
    "\n",
    "tokens_used = 0\n",
    "last_check_time = time.time()\n",
    "TPM_LIMIT = 200000\n",
    "\n",
    "def gpt35_turbo_inference(prompt, text, model=\"gpt-4o-mini\", max_tokens=1000, temperature=0.7):\n",
    "    global tokens_used, last_check_time\n",
    "    \n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - last_check_time\n",
    "\n",
    "    if elapsed_time > 60:\n",
    "        tokens_used = 0\n",
    "        last_check_time = current_time\n",
    "\n",
    "    combined_prompt = f\"{text}\\n\\n{prompt}\"\n",
    "    tokens_estimate = len(combined_prompt.split()) + max_tokens\n",
    "    \n",
    "    if tokens_used + tokens_estimate > TPM_LIMIT:\n",
    "        sleep_time = 60 - (current_time - last_check_time)\n",
    "        print(f\"Rate limit approaching. Sleeping for {sleep_time:.2f} seconds...\")\n",
    "        time.sleep(sleep_time)\n",
    "        tokens_used = 0\n",
    "        last_check_time = time.time()\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": combined_prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        tokens_used += response.usage['total_tokens']\n",
    "        return response.choices[0].message['content'].strip(), tokens_used\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None, tokens_used\n",
    "\n",
    "\n",
    "def process_files(directory, prompt, output_directory, progress_file):\n",
    "    processed_files = set()\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as file:\n",
    "            processed_files.update(file.read().splitlines())\n",
    "\n",
    "    total_files = [f for f in os.listdir(directory) if f.endswith(\".txt\")]\n",
    "    total_count = len(total_files)\n",
    "    processed_count = len(processed_files)\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for filename in total_files:\n",
    "        if filename in processed_files:\n",
    "            print(f\"Skipping {filename}, already processed.\")\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                result, tokens_used = gpt35_turbo_inference(prompt, text)\n",
    "                if result:\n",
    "                    save_result(filename, result, output_directory)\n",
    "                    print(f\"Processed and saved {filename}\")\n",
    "                    processed_count += 1\n",
    "                    with open(progress_file, 'a') as pfile:\n",
    "                        pfile.write(f\"{filename}\\n\")\n",
    "                else:\n",
    "                    print(f\"Failed to process {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "        print(f\"Progress: {processed_count}/{total_count} files processed.\")\n",
    "\n",
    "def save_result(filename, result, output_directory):\n",
    "    output_file_path = os.path.join(output_directory, filename)\n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(result)\n",
    "        print(f\"Output saved to {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to {output_file_path}: {e}\")\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Process text files with GPT-4o-mini.\")\n",
    "    parser.add_argument('--input_directory', type=str, required=True, help=\"Directory containing input text files\")\n",
    "    parser.add_argument('--output_directory', type=str, required=True, help=\"Directory to save output text files\")\n",
    "    parser.add_argument('--progress_file', type=str, help=\"File to record progress of processed files\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if not args.progress_file:\n",
    "        args.progress_file = os.path.join(args.output_directory, 'progress.txt')\n",
    "\n",
    "    \n",
    "\n",
    "    prompt = (\n",
    "        \"When generating a new question and answer pair, creatively rewrite the expressions of the existing QA pairs to enhance data diversity. Ensure that all key technical information and details are preserved within the QA structure, while making the language more varied and engaging.\\nSteps:\\n1.Carefully read the existing QA pair to understand its core information and technical details.\\n2.Reformulate the question, changing the angle of inquiry or using different vocabulary, but ensuring that all essential information is preserved.\\n3.Ensure that the answer is detailed and accurate, while moderately adjusting the language to improve readability and appeal.\\n4.The generated QA pair should be presented in a clear and concise format: '\\nQuestion: \\nAnswer:'.\"\n",
    "    )\n",
    "    process_files(args.input_directory, prompt, args.output_directory, args.progress_file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
